---
title: "Process Data"
jupyter: julia-1.9
execute:
    echo: false
---

```{julia}
#| output: false
using NCDatasets
using DataStructures
using Dates
using GLM
using Turing
using Optim
using DynamicHMC
using MultivariateStats
using StatsBase
using Plots
using DataFrames
```

```{julia}
# This code block was just to figure out what I was doing mostly, the next is for the general case
# Please ignore random semicolons. I kept getting weird outputs in my quarto so I had to deal with this.
# dummy case: read in 1 data file
temp_1979_ds = NCDataset("data/raw/2m_temperature_1979.nc");
temp_1980_ds = NCDataset("data/raw/2m_temperature_1980.nc");

# gridcells are 66x27. we need to pick the ones that constitute the texas area
# found these indices by inspecting data
lat_lower = 27
lat_upper = 14
lon_left = 24
lon_right = 38

# select only texas gridcells
temp_1979 = temp_1979_ds["t2m"][lon_left:lon_right,lat_upper:lat_lower,:]
time_1979 = temp_1979_ds["time"][:]
temp_1980 = temp_1980_ds["t2m"][lon_left:lon_right,lat_upper:lat_lower,:]
time_1980 = temp_1980_ds["time"][:]

# join matrices
temp_joined = cat(temp_1979, temp_1980, dims=3);
time_joined = cat(time_1979, time_1980, dims=1);
```

```{julia}
# now, we'll do this iteratively for all years
datasets = OrderedDict()

for year in 1979:2022
    datasets["temp_$(year)_ds"] = NCDataset("data/raw/2m_temperature_$(year).nc");
end

temp_joined = datasets["temp_1979_ds"]["t2m"][lon_left:lon_right,lat_upper:lat_lower,:];
time_joined = datasets["temp_1979_ds"]["time"][:];

for (key, dataset) in datasets
    # skip the first dataset since it's already in the joined variable
    if key == "temp_1979_ds"
        continue
    end
    # pick Texas out of the current dataset and add it to the joined dataset, also join time
    temp_current = dataset["t2m"][lon_left:lon_right,lat_upper:lat_lower,:]
    time_current = dataset["time"][:]
    temp_joined = cat(temp_joined, temp_current, dims=3)
    time_joined = cat(time_joined, time_current, dims=1)
end

# reshape temp_joined to have 24 time steps per column
temp_reshaped = reshape(temp_joined, size(temp_joined, 1), size(temp_joined, 2), 24, :)
# take the mean of each 24-step block along the 3rd dimension
temp_averaged = mean(temp_reshaped, dims=3)
# reshape back to original shape
temp_joined_averaged = reshape(temp_averaged, size(temp_joined, 1), size(temp_joined, 2), :);

time_joined_dates = unique([Dates.Date(x) for x in time_joined])
time_len = length(time_joined_dates)

temp_joined_averaged = reverse(temp_joined_averaged; dims=2);
```

```{julia}
# Subset the precip_tx data to be the same timescale (1979-2022)
precip_tx = NCDataset("data/raw/precip_tx.nc");
precip_tx_joined = precip_tx["precip"][:,:,1:time_len]
precip_lon = precip_tx["lon"][:]
precip_lat = precip_tx["lat"][:]
precip_lat = reverse(precip_lat)
precip_tx_joined = reverse(precip_tx_joined; dims=2);
```

Define the functions we'll use:
```{julia}

# preprocess functions by getting climatology--same as lab 6
function preprocess(temp::Array{T,3}, temp_ref::Array{T,3})::AbstractMatrix where {T}
    n_lon, n_lat, n_t = size(temp)
    climatology = mean(temp_ref; dims=3)
    temp_anom = temp .- climatology

    # reshape to 2D
    temp_anom = reshape(temp_anom, n_lon * n_lat, n_t)

    # strip the units
    return temp_anom
end


# helper functions for knn
function euclidean_distance(x::AbstractVector, y::AbstractVector)::AbstractFloat
    return sqrt(sum((x .- y) .^ 2))
end

function nsmallest(x::AbstractVector, n::Int)::Vector{Int}
    idx = sortperm(x)
    return idx[1:n]
end



# calculate knn for a feature matrix with X_i input. From lab 6
function knn(X::AbstractMatrix, X_i::AbstractVector, K::Int)::Tuple{Int,AbstractVector}
    # calculate the distances between X_i and each row of X
    dist = [euclidean_distance(X_i, X[j, :]) for j in 1:size(X, 1)]
    idx = nsmallest(dist, K)
    w = 1 ./ dist[idx]
    w ./= sum(w)
    idx_sample = sample(idx, Weights(w))
    return (idx_sample, vec(X[idx_sample, :]))
end

# do the whole knn analysis with our data. From lab 6
function predict_knn(temp_train, temp_test, precip_train; n_pca::Int)
    X_train = preprocess(temp_train, temp_train)
    X_test = preprocess(temp_test, temp_train)

    # fit the PCA model to the training data
    pca_model = fit(PCA, X_train; maxoutdim=n_pca)

    # project the test data onto the PCA basis
    train_embedded = predict(pca_model, X_train)
    test_embedded = predict(pca_model, X_test)

    # use the `knn` function for each point in the test data
    precip_pred = map(1:size(X_test, 2)) do i
        idx, _ = knn(train_embedded', test_embedded[:, i], 3)
        precip_train[:, :, idx]
    end

    # return a matrix of predictions
    return precip_pred
end

# This was part of my attempt at a linear regression like we learned in class, but unfortunately a statistical approach caused too many errors that I couldn't figure out.
# @model function poisson_regression(y::AbstractVector, x::AbstractVector)
#     # priors
#     α ~ Normal(0, 5)
#     β ~ Normal(0, 5)

#     # likelihood
#     λ = @. exp(α + β * x)
#     return y .~ Poisson.(λ)
# end

# Linear regression function. Similar to predict_knn, but do a linear regression instead of knn
function predict_lm(temp_train, precip_train;)
    X_train = preprocess(temp_train, temp_train)

    # fit the PCA model to the training data
    # I'm only using 1 PC since it explains the most variance and can be used as regression for all the timesteps for each gridcell.
    # The scree plot elbow was roughly after the 1st or 2nd PC, so this should account for a lot of the variance.
    pca_model = fit(PCA, X_train; maxoutdim=1)

    # project the test data onto the PCA basis
    train_embedded = predict(pca_model, X_train)

    # now we have PC vlues for training/testing data. For the gridcell in precip, let's do a linear model using our PCA as a predictor.

    # need to make a dataframe for lm syntax
    df = DataFrame(x=vec(train_embedded'), y=Float64.(precip_train))
    reg_model = lm(@formula(y ~ x), df)
end
```

# Exploratory Data Analysis

Now, I'll do a PCA-KNN analysis like we did in Lab 6. I'll detail the methods more in the relevant section. Below is the results from 3 randomly selected days of this PCA-KNN analysis.

```{julia}
# Split the data into training/testing
idx_partition = findfirst(time_joined_dates .== time_joined_dates[end] - Dates.Year(10))
train_idx = 1:idx_partition
test_idx = (idx_partition+1):time_len

precip_train = precip_tx_joined[:, :, train_idx]
precip_test = precip_tx_joined[:, :, test_idx]
temp_train = temp_joined_averaged[:, :, train_idx]
temp_test = temp_joined_averaged[:, :, test_idx]
time_train = time_joined_dates[train_idx]
time_test = time_joined_dates[test_idx]

# preprocess the data for climatology
n_lon, n_lat, n_t = size(temp_joined_averaged)

# predict KNN for 3 random days from the testing data
t_sample = rand(1:size(temp_test, 3), 3)
precip_pred = predict_knn(temp_train, temp_test[:, :, t_sample], precip_train; n_pca=3)

mse_pcaknn = [0.0,0.0,0.0]
# plot the difference between predicted precipitation and actual precipitation in the test set
p = map(eachindex(t_sample)) do ti
    t = t_sample[ti]
    y_pred = precip_pred[ti]'
    y_actual = precip_test[:, :, t]'
    mse_pcaknn[ti] = (mean(skipmissing((y_actual .- y_pred).^2)))
    cmax = max(maximum(skipmissing(y_pred)), maximum(skipmissing(y_actual)))

    p1 = heatmap(
        precip_lon,
        precip_lat,
        y_pred;
        xlabel="Longitude",
        ylabel="Latitude",
        title="Predicted",
        aspect_ratio=:equal,
        clims=(0, cmax)
    )
    p2 = heatmap(
        precip_lon,
        precip_lat,
        y_actual;
        xlabel="Longitude",
        ylabel="Latitude",
        title="Actual",
        aspect_ratio=:equal,
        clims=(0, cmax)
    )
    plot(p1, p2; layout=(2, 1), size=(1000, 400))
end
plot(p...; layout=(2, 3), size=(1500, 1200))
```

Here is a plot for linear regression. I was able to implement an approach that did linear regression on the principal components to a randomly selected gridcell in the precipitation data.

```{julia}

# x1, x2 are the coordinates for the precip gridcell of choice, I'll just do 5,5 as an example
x1 = rand(1:24)
x2 = rand(1:24)
precip_gridcell = precip_train[x1, x2, :]
# need to get rid of missing values in precip, so I'll replace them with 0 here. This probably negatively affects my fit by quite a bit, but this is the problem with linear regresion.
precip_gridcell = replace(precip_gridcell, missing => 0)

#reg_model = predict_lm(temp_train, precip_gridcell)

X_train = preprocess(temp_train, temp_train)
pca_model = fit(PCA, X_train; maxoutdim=1)

# project the test data onto the PCA basis
train_embedded = predict(pca_model, X_train)

# now we have PC vlues for training/testing data. For the gridcell in precip, let's do a linear model using our PCA as a predictor.

# need to make a dataframe for lm syntax
df = DataFrame(x=vec(train_embedded'), y=Float64.(precip_gridcell))
reg_model = lm(@formula(y ~ x), df)

# now, create the test input
X_test = preprocess(temp_test, temp_train)
test_embedded = predict(pca_model, X_test)

precip_pred = predict(reg_model, DataFrame(x=vec(test_embedded)))
plot(time_test, precip_pred, label="Predicted")
plot!(time_test, precip_test[x1, x2, :], label="Actual")
```

Here are some metrics that see how good my fit was for these examples, and also justify the number of PCA components.

```{julia}
# create a scree plot to justify the number of PC's used
pca_model = fit(PCA, preprocess(temp_train, temp_train); maxoutdim=25, pratio=0.999);
plot(
    principalvars(pca_model) / var(pca_model);
    xlabel="# of PCs",
    ylabel="Fraction of Variance Explained",
    label=false,
    title="Variance Explained"
)
```
Notice the elbow of the scree plot is pretty much at PC=2. The first principle component explains over 90% of the variance of the data, so using only 2 PCs for KNN is a pretty valid choice as is using only 1 PC for the regression for ease of coding.

Here are the residuals for my linear fit. I'll comment on these in the model comparison section, but it's pretty apparent that they're correlated with my x values.
```{julia}
res_values = residuals(reg_model)
plot(
    time_train,
    res_values,
    seriestype=:scatter,
    xlabel="Time",
    ylabel="Residual Value",
    label="Residuals"
    )
```

Next I'll calculate the difference in MSE between the KNN method and the linear model (postive means that the KNN model has a higher MSE).

```{julia}
# calculate MSE for PCA by taking the mean of the 3 timesteps
mse_avg_knn = mean(mse_pcaknn)

# calculate MSE for linear model
mse_lm = mean((precip_test[x1, x2, :] .- precip_pred).^2)

mse_diff = mse_avg_knn - mse_lm
print(mse_diff)
```

Sometimes, the MSE is higher for the PCA-KNN approach, or the MSE might be higher for the linear regression approach. This varies based on the gridcell in precipitation that I'm running the linear model on or the time slice of precipitation I'm running PCA-KNN on. It's hard to quantify the true MSE of each of these unless I did comprehensive sampling of the data at each gridcell or timestep. This could be a source of improvement on this report, but I struggled mightily to implement the simple cases herein and doing a more comprehensive quantification of MSE would be beyond my current scope.

# Methods
In both methods, I used PCA on the temperature data to reduce the dimensions down from space and time just to principle component space. This was the overarching theme between both KNN and linear regression, as I found this was the most powerful way to do "downscaling" by removing the dimensions of the coarse temperature data, thus allowing me to conduct some kind of modeling with the finer precipitation data. For PCA, I used a train/test split similar to the last lab, where the last 10 years were my testing data. I used data from 1979-2022, as I wanted to have at least 30 years of training data.

## PCA-KNN
This method used a non-parametric approach with several steps.  In this case, I used KNN in the principal components space to choose the most important time steps from the temperature data. I then used that to select the corresponding time steps in the precipitation data, thus mapping coarse temperature data to finer precipitation data. This process was done for 3 random days to choose as the X_i in KNN. As evidenced by the output graphs, the actual precipitation values were often very different from the others. In terms of hyperparameter choices, I made a few decisions. The scree plot showed a significant elbow in the first couple principal components, so I deemed it reasonable to just use 3. I used a modest K-value of 3 to decrease the model bias, but this also caused high variance between runs on different days. 

## Linear Regression
This method used the Julia package, GLM.jl, to create a generalized linear model between the temperature data principal components and a specific gridcell of the precipitation data. The temperature data principal components allow for the difference in dimensions between the datasets to be irrelevant; however, the major limitation of only examining one gridcell limits how useful this method is. Additionally, this linear model does not use the Bayesian approaches that we learned in class based on log likelihood or a priori estimates, and as far as I can tell, also has little to do with the distributions of our data. I attempted to implement a more statistics-based approach, but I couldn't solve the multitude of errors it brought, so I was forced to choose a simpler linear model. Given precipitation's extreme non-linearity and also its tendencies to have values of 0, this generally had a very high MSE/obviously did not fit the data well at all.

# Model Comparison

As we can see from the MSE changing wildly for each model, it's hard to say which model is better. I'd instead consider the use cases for each model. The KNN approach is exceptionally useful for analyzing the difference at a given time on a regional scale between reconstructed precipitation and the actual precipitation measurements. Given that we're only conditioning on temperature, which is only a piece of the story of precipitation, I'd expect it not to reconstruct it very well. However, this could be useful to constrain at least some effect of temperature on regional precipitation, even when we have only a coarse regional temperature data product.

The use case for linear regression is also something I see as a big advantage. Precipitation is controlled by many factors, and if more are included as predictors, it's possible that the regression could become more robust. Being able to predict precipitation trend at a more local location is a valuable tool for those who might live there or a company interested in a specific location. It would be interesting to analyze gauge data within the smaller gridcells and see how it compares to both the observed gridcell value and the predicted gridcell value. A major limitation, however, is the inability to properly simulate extreme values. A better linear regression such as a polynomial fit could solve this problem, but my linear regression model was only able to do mostly a simpler linear fit. We can see in the plot of the residuals that they are highly correlated with our predictor. This means our fit is really bad and we need to get a better one in order to make any kind of predictions.

Overall, both models are interesting for analyzing specific trends and have their own use cases, but are both pretty bad and need major improvements, which is what I'd expect given I'm trying to do a lot (predict fine precipitation values) with very little (coarse temperature data).

# Conclusion
This is a pretty complicated problem. It feels like the use case for each model should be considered carefully before choosing one that is appropriate for using on gridded data, such as whether you're predicting one gridcell or a regional scale. Simple linear regression methods look like they're not even close to good enough, while PCA-KNN approaches capture some information at a regional scale but are also often inaccurate. Overall, it seems like much, much work should be added for either of my models to make them better and more accurately represent the precipitation data.



```{julia}
#|eval: false
#|echo: false

# Below was my best attempt at doing a quantile-quantile model for downscaling. I didn't include it as I couldn't get it to work with downscaling, but I include it here to show some of the work I put into this project.

# define a function to flatten 3-D data
# as a note: this was annoying to figure out in the last lab, and I think it'd be a good candidate for code to just give us in the lab
function flatten_3d(data3d)
    n_lon, n_lat, n_t = size(data3d)
    reshaped = reshape(data3d, n_lon * n_lat, n_t) 
    return reshaped
end

# helper function to find the index of the precipitation value mapped to an input temperature value
function get_quantile_index(temp_val, temp_q)
    # Find the index of the temperature quantile
    idx = findfirst(x -> x >= temp_val, temp_q)
    return idx
end

# get gridcells on an axis
temp_train_rs = flatten_3d(temp_train)
precip_train_rs = flatten_3d(precip_train)
# precip_train_rs = replace(precip_train_rs, missing => 0)

# for now, dummy case at first timestep
temp_quantiles = quantile(temp_train_rs[:,1], 0:0.01:1)
precip_quantiles = quantile(skipmissing(precip_train_rs[:,1]), 0:0.01:1)

precip_mapped = similar(precip_train_rs[:,1])

for (idx, t) in enumerate(temp_train_rs[:,1], precip_train)
    q_idx = get_quantile_index(t,temp_quantiles)
    # select the mapped quantile given its index
    val = precip_quantiles[q_idx]
    # case to deal with the missing coastline data
    if precip_train[idx] == missing:
        continue
    precip_mapped[idx] = val
end

# # unflatten the precip_mapped data
precip_mapped_regridded = reshape(precip_mapped, (24, 24))
precip_mapped_regridded = replace(precip_mapped_regridded, NaN => missing)
cmax = max(maximum(skipmissing(precip_mapped_regridded)), maximum(skipmissing(precip_train[:,:,1])))

# p1 = heatmap(
#     precip_lon,
#     precip_lat,
#     precip_mapped_regridded';
#     xlabel="Longitude",
#     ylabel="Latitude",
#     title="Predicted",
#     aspect_ratio=:equal,
#     clims=(0, cmax)
# )

# p2 = heatmap(
#     precip_lon,
#     precip_lat,
#     precip_train[:,:,1]';
#     xlabel="Longitude",
#     ylabel="Latitude",
#     title="Actual",
#     aspect_ratio=:equal,
#     clims=(0, cmax)
# ) 
# plot(p1, p2; layout=(2, 1), size=(1000, 400))
```